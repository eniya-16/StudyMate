{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOAKcyULcyfM+h3AFOtrDok"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"4bde3eff13ab4a28bde31de78db49ac1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d6b71beea322426c9b76a820ca52c29f","IPY_MODEL_9802bf0c794f4d76a82d7eea2a429215","IPY_MODEL_1d810d09527d4d2f98c9cfa4b1def63c"],"layout":"IPY_MODEL_565583a03e7f4ab38d3c2e61268eff64"}},"d6b71beea322426c9b76a820ca52c29f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_af99a9551bc64fdeaf215da966f8ebdc","placeholder":"â€‹","style":"IPY_MODEL_70f50c7ecc0540159c9fa06a741d8654","value":"Loadingâ€‡checkpointâ€‡shards:â€‡100%"}},"9802bf0c794f4d76a82d7eea2a429215":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_167c13bc15cc4bda8e88a2c0aed3f788","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f8885b9312a5441d819ddfbed473be3a","value":2}},"1d810d09527d4d2f98c9cfa4b1def63c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ddbcf87c43b0482f988cf1bb57a1efcd","placeholder":"â€‹","style":"IPY_MODEL_62488765775a4a8bbf6abba5cde7a133","value":"â€‡2/2â€‡[00:18&lt;00:00,â€‡â€‡7.67s/it]"}},"565583a03e7f4ab38d3c2e61268eff64":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"af99a9551bc64fdeaf215da966f8ebdc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"70f50c7ecc0540159c9fa06a741d8654":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"167c13bc15cc4bda8e88a2c0aed3f788":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f8885b9312a5441d819ddfbed473be3a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ddbcf87c43b0482f988cf1bb57a1efcd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"62488765775a4a8bbf6abba5cde7a133":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","source":["# ===============================\n","# ðŸ“˜ StudyMate â€” Direct PDF Answers (IBM Granite) + Summary, Key Points & MCQs\n","# ===============================\n","\n","!apt-get -y install poppler-utils tesseract-ocr\n","!pip install -q transformers gradio accelerate sentence-transformers faiss-cpu pymupdf pytesseract pillow\n","\n","import gradio as gr\n","import fitz\n","import faiss\n","import numpy as np\n","from sentence_transformers import SentenceTransformer\n","from transformers import pipeline\n","import pytesseract\n","from PIL import Image\n","import io\n","import traceback\n","\n","# === Load models ===\n","embedder = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n","pipe = pipeline(\n","    \"text-generation\",\n","    model=\"ibm-granite/granite-3.3-2b-instruct\",\n","    device_map=\"auto\"\n",")\n","\n","# === Helper functions ===\n","def chunk_text(text, chunk_size_words=250, overlap_words=50):\n","    words = text.split()\n","    chunks, start = [], 0\n","    while start < len(words):\n","        end = min(start + chunk_size_words, len(words))\n","        chunk = \" \".join(words[start:end]).strip()\n","        if chunk:\n","            chunks.append(chunk)\n","        start += chunk_size_words - overlap_words\n","    return chunks\n","\n","def _normalize(embs: np.ndarray):\n","    embs = np.array(embs, dtype=\"float32\")\n","    norms = np.linalg.norm(embs, axis=1, keepdims=True)\n","    norms[norms == 0] = 1.0\n","    return embs / norms\n","\n","def extract_text_from_pdf(path, status_callback=None):\n","    doc = fitz.open(path)\n","    texts = []\n","    n_pages = len(doc)\n","    for i, page in enumerate(doc):\n","        if status_callback:\n","            status_callback(f\"Processing page {i+1}/{n_pages}...\")\n","        text = page.get_text(\"text\")\n","        if text.strip():\n","            texts.append(text.replace(\"\\n\", \" \").strip())\n","        else:\n","            pix = page.get_pixmap()\n","            img = Image.open(io.BytesIO(pix.tobytes(\"png\")))\n","            ocr_text = pytesseract.image_to_string(img)\n","            if ocr_text.strip():\n","                texts.append(ocr_text.replace(\"\\n\", \" \").strip())\n","    return texts\n","\n","# === Global state ===\n","doc_chunks = []\n","doc_embeddings = None\n","index = None\n","mcq_state = {}  # stores last MCQ correct option\n","\n","# === Process PDFs ===\n","def process_pdfs(pdf_filepaths, progress=gr.Progress()):\n","    global doc_chunks, doc_embeddings, index\n","    try:\n","        if not pdf_filepaths:\n","            return \"âŒ No file uploaded.\", []\n","        pdf_paths = [pdf_filepaths] if isinstance(pdf_filepaths, str) else list(pdf_filepaths)\n","\n","        texts = []\n","        for path in pdf_paths:\n","            texts.extend(extract_text_from_pdf(path, status_callback=progress))\n","\n","        if not texts:\n","            return \"âŒ Could not extract text (even with OCR).\", []\n","\n","        full_text = \" \".join(texts)\n","        doc_chunks = chunk_text(full_text, chunk_size_words=250, overlap_words=50)\n","\n","        doc_embeddings = embedder.encode(\n","            doc_chunks,\n","            convert_to_numpy=True,\n","            show_progress_bar=True,\n","            batch_size=64\n","        )\n","        doc_embeddings = _normalize(doc_embeddings).astype(\"float32\")\n","\n","        dim = doc_embeddings.shape[1]\n","        index = faiss.IndexFlatIP(dim)\n","        index.add(doc_embeddings)\n","\n","        return f\"âœ… Processed {len(pdf_paths)} PDF(s). Indexed {len(doc_chunks)} chunks.\", []\n","    except Exception as e:\n","        return f\"âŒ Error: {e}\\n\\n{traceback.format_exc()}\", []\n","\n","# === Answer Questions + Summary + Key Points + MCQs ===\n","def answer_question(query, history):\n","    global doc_chunks, index, mcq_state\n","    try:\n","        if index is None or not doc_chunks:\n","            history = history or []\n","            history.append((\"System\", \"âš  Please upload and process PDFs first.\"))\n","            return history, \"\"\n","\n","        # Check if user is answering previous MCQ\n","        if mcq_state.get(\"correct_option\"):\n","            selected = query.strip().lower()\n","            correct = mcq_state[\"correct_option\"].strip().lower()\n","            if selected == correct:\n","                reply = \"âœ… Correct!\"\n","            else:\n","                reply = \"âŒ Wrong!\"\n","            mcq_state = {}\n","            history = history or []\n","            history.append((f\"You answered: {query}\", reply))\n","            return history, \"\"\n","\n","        # Embed query\n","        q_emb = embedder.encode([query], convert_to_numpy=True)\n","        q_emb = _normalize(q_emb).astype(\"float32\")\n","\n","        k = min(7, len(doc_chunks))\n","        D, I = index.search(q_emb, k)\n","        retrieved = [doc_chunks[i] for i in I[0] if 0 <= i < len(doc_chunks)]\n","\n","        if not retrieved:\n","            history = history or []\n","            history.append((query, \"âš  No relevant passages found.\"))\n","            return history, \"\"\n","\n","        context = \"\\n\\n\".join(retrieved)\n","        lower_query = query.lower()\n","\n","        # Summarization\n","        if \"summarize\" in lower_query:\n","            prompt = f\"Summarize concisely:\\n\\n{context}\\n\\nSummary:\"\n","        # Key points\n","        elif \"important points\" in lower_query or \"key points\" in lower_query:\n","            prompt = f\"List important points from the content:\\n\\n{context}\\n\\nPoints:\"\n","        # MCQ generation\n","        elif \"mcq\" in lower_query or \"quiz\" in lower_query:\n","            prompt = (\n","                f\"Create 2-3 multiple choice questions (with options a, b, c, d) from the following content. \"\n","                f\"Provide the correct option for each question in the format 'Question X: correct_option'.\\n\\n{context}\\n\\nMCQs:\"\n","            )\n","            resp = pipe(prompt, max_new_tokens=300, do_sample=False, temperature=0.0)\n","            output_text = resp[0][\"generated_text\"].strip()\n","            # Save first question correct answer\n","            lines = output_text.split(\"\\n\")\n","            for line in lines:\n","                if \"Question 1:\" in line and \":\" in line:\n","                    mcq_state[\"correct_option\"] = line.split(\":\")[-1].strip()\n","                    break\n","            history = history or []\n","            history.append((query, output_text))\n","            return history, \"\"\n","        else:\n","            # Normal Q&A\n","            prompt = (\n","                \"Answer the question *directly and concisely* using ONLY the context below. \"\n","                \"If answer not present, reply 'I don't know'.\\n\\n\"\n","                f\"Context:\\n{context}\\n\\nQuestion:{query}\\nAnswer:\"\n","            )\n","            resp = pipe(prompt, max_new_tokens=150, do_sample=False, temperature=0.0)\n","            output_text = resp[0].get(\"generated_text\", \"\").strip()\n","            if output_text.startswith(prompt):\n","                output_text = output_text[len(prompt):].strip()\n","\n","        history = history or []\n","        history.append((query, output_text))\n","        return history, \"\"\n","    except Exception as e:\n","        history = history or []\n","        history.append((\"System\", f\"âš  Error: {e}\\n\\n{traceback.format_exc()}\"))\n","        return history, \"\"\n","\n","# === Gradio UI ===\n","with gr.Blocks() as demo:\n","    gr.Markdown(\"# ðŸ“˜ StudyMate â€” IBM Granite + Summary + Key Points + MCQs\")\n","\n","    with gr.Row():\n","        pdf_input = gr.File(\n","            label=\"Upload PDFs\",\n","            file_count=\"multiple\",\n","            file_types=[\".pdf\"],\n","            type=\"filepath\",\n","        )\n","        process_btn = gr.Button(\"Process PDFs\")\n","\n","    status = gr.Textbox(label=\"Status\", interactive=False)\n","    chatbot = gr.Chatbot(label=\"StudyMate Chat\", height=500)\n","    state = gr.State([])\n","\n","    with gr.Row():\n","        msg = gr.Textbox(placeholder=\"Ask a question...\", label=\"Your question\", lines=2)\n","        send = gr.Button(\"Send\")\n","        clear = gr.Button(\"Clear\")\n","\n","    process_btn.click(process_pdfs, inputs=[pdf_input], outputs=[status, state])\n","    msg.submit(answer_question, inputs=[msg, state], outputs=[chatbot, msg])\n","    send.click(answer_question, inputs=[msg, state], outputs=[chatbot, msg])\n","\n","    def _clear_all():\n","        return [], \"\", \"\", []\n","    clear.click(_clear_all, None, [chatbot, msg, status, state])\n","\n","demo.launch(share=True)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":799,"referenced_widgets":["4bde3eff13ab4a28bde31de78db49ac1","d6b71beea322426c9b76a820ca52c29f","9802bf0c794f4d76a82d7eea2a429215","1d810d09527d4d2f98c9cfa4b1def63c","565583a03e7f4ab38d3c2e61268eff64","af99a9551bc64fdeaf215da966f8ebdc","70f50c7ecc0540159c9fa06a741d8654","167c13bc15cc4bda8e88a2c0aed3f788","f8885b9312a5441d819ddfbed473be3a","ddbcf87c43b0482f988cf1bb57a1efcd","62488765775a4a8bbf6abba5cde7a133"]},"id":"vGgT56u3UiIH","executionInfo":{"status":"ok","timestamp":1758342441760,"user_tz":-330,"elapsed":50130,"user":{"displayName":"Eniya V.G","userId":"03792943113852844754"}},"outputId":"7786c6f5-646c-455f-e2d2-13bb5eba332e"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","tesseract-ocr is already the newest version (4.1.1-2.1build1).\n","poppler-utils is already the newest version (22.02.0-2ubuntu0.10).\n","0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n"]},{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4bde3eff13ab4a28bde31de78db49ac1"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Device set to use cuda:0\n","/tmp/ipython-input-1805429270.py:198: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n","  chatbot = gr.Chatbot(label=\"StudyMate Chat\", height=500)\n"]},{"output_type":"stream","name":"stdout","text":["Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n","* Running on public URL: https://39629652a55d5606cc.gradio.live\n","\n","This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div><iframe src=\"https://39629652a55d5606cc.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":[]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":[],"metadata":{"id":"rHGbIYnpUiRR"},"execution_count":null,"outputs":[]}]}