# -*- coding: utf-8 -*-
"""eniya.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eQfaxwGrA9JttsJymmMU8DAnZEUhaRpi
"""

# ===============================
# ðŸ“˜ StudyMate â€” Direct PDF Answers (IBM Granite) + Summary, Key Points & MCQs
# ===============================

!apt-get -y install poppler-utils tesseract-ocr
!pip install -q transformers gradio accelerate sentence-transformers faiss-cpu pymupdf pytesseract pillow

import gradio as gr
import fitz
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer
from transformers import pipeline
import pytesseract
from PIL import Image
import io
import traceback

# === Load models ===
embedder = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")
pipe = pipeline(
    "text-generation",
    model="ibm-granite/granite-3.3-2b-instruct",
    device_map="auto"
)

# === Helper functions ===
def chunk_text(text, chunk_size_words=250, overlap_words=50):
    words = text.split()
    chunks, start = [], 0
    while start < len(words):
        end = min(start + chunk_size_words, len(words))
        chunk = " ".join(words[start:end]).strip()
        if chunk:
            chunks.append(chunk)
        start += chunk_size_words - overlap_words
    return chunks

def _normalize(embs: np.ndarray):
    embs = np.array(embs, dtype="float32")
    norms = np.linalg.norm(embs, axis=1, keepdims=True)
    norms[norms == 0] = 1.0
    return embs / norms

def extract_text_from_pdf(path, status_callback=None):
    doc = fitz.open(path)
    texts = []
    n_pages = len(doc)
    for i, page in enumerate(doc):
        if status_callback:
            status_callback(f"Processing page {i+1}/{n_pages}...")
        text = page.get_text("text")
        if text.strip():
            texts.append(text.replace("\n", " ").strip())
        else:
            pix = page.get_pixmap()
            img = Image.open(io.BytesIO(pix.tobytes("png")))
            ocr_text = pytesseract.image_to_string(img)
            if ocr_text.strip():
                texts.append(ocr_text.replace("\n", " ").strip())
    return texts

# === Global state ===
doc_chunks = []
doc_embeddings = None
index = None
mcq_state = {}  # stores last MCQ correct option

# === Process PDFs ===
def process_pdfs(pdf_filepaths, progress=gr.Progress()):
    global doc_chunks, doc_embeddings, index
    try:
        if not pdf_filepaths:
            return "âŒ No file uploaded.", []
        pdf_paths = [pdf_filepaths] if isinstance(pdf_filepaths, str) else list(pdf_filepaths)

        texts = []
        for path in pdf_paths:
            texts.extend(extract_text_from_pdf(path, status_callback=progress))

        if not texts:
            return "âŒ Could not extract text (even with OCR).", []

        full_text = " ".join(texts)
        doc_chunks = chunk_text(full_text, chunk_size_words=250, overlap_words=50)

        doc_embeddings = embedder.encode(
            doc_chunks,
            convert_to_numpy=True,
            show_progress_bar=True,
            batch_size=64
        )
        doc_embeddings = _normalize(doc_embeddings).astype("float32")

        dim = doc_embeddings.shape[1]
        index = faiss.IndexFlatIP(dim)
        index.add(doc_embeddings)

        return f"âœ… Processed {len(pdf_paths)} PDF(s). Indexed {len(doc_chunks)} chunks.", []
    except Exception as e:
        return f"âŒ Error: {e}\n\n{traceback.format_exc()}", []

# === Answer Questions + Summary + Key Points + MCQs ===
def answer_question(query, history):
    global doc_chunks, index, mcq_state
    try:
        if index is None or not doc_chunks:
            history = history or []
            history.append(("System", "âš  Please upload and process PDFs first."))
            return history, ""

        # Check if user is answering previous MCQ
        if mcq_state.get("correct_option"):
            selected = query.strip().lower()
            correct = mcq_state["correct_option"].strip().lower()
            if selected == correct:
                reply = "âœ… Correct!"
            else:
                reply = "âŒ Wrong!"
            mcq_state = {}
            history = history or []
            history.append((f"You answered: {query}", reply))
            return history, ""

        # Embed query
        q_emb = embedder.encode([query], convert_to_numpy=True)
        q_emb = _normalize(q_emb).astype("float32")

        k = min(7, len(doc_chunks))
        D, I = index.search(q_emb, k)
        retrieved = [doc_chunks[i] for i in I[0] if 0 <= i < len(doc_chunks)]

        if not retrieved:
            history = history or []
            history.append((query, "âš  No relevant passages found."))
            return history, ""

        context = "\n\n".join(retrieved)
        lower_query = query.lower()

        # Summarization
        if "summarize" in lower_query:
            prompt = f"Summarize concisely:\n\n{context}\n\nSummary:"
        # Key points
        elif "important points" in lower_query or "key points" in lower_query:
            prompt = f"List important points from the content:\n\n{context}\n\nPoints:"
        # MCQ generation
        elif "mcq" in lower_query or "quiz" in lower_query:
            prompt = (
                f"Create 2-3 multiple choice questions (with options a, b, c, d) from the following content. "
                f"Provide the correct option for each question in the format 'Question X: correct_option'.\n\n{context}\n\nMCQs:"
            )
            resp = pipe(prompt, max_new_tokens=300, do_sample=False, temperature=0.0)
            output_text = resp[0]["generated_text"].strip()
            # Save first question correct answer
            lines = output_text.split("\n")
            for line in lines:
                if "Question 1:" in line and ":" in line:
                    mcq_state["correct_option"] = line.split(":")[-1].strip()
                    break
            history = history or []
            history.append((query, output_text))
            return history, ""
        else:
            # Normal Q&A
            prompt = (
                "Answer the question *directly and concisely* using ONLY the context below. "
                "If answer not present, reply 'I don't know'.\n\n"
                f"Context:\n{context}\n\nQuestion:{query}\nAnswer:"
            )
            resp = pipe(prompt, max_new_tokens=150, do_sample=False, temperature=0.0)
            output_text = resp[0].get("generated_text", "").strip()
            if output_text.startswith(prompt):
                output_text = output_text[len(prompt):].strip()

        history = history or []
        history.append((query, output_text))
        return history, ""
    except Exception as e:
        history = history or []
        history.append(("System", f"âš  Error: {e}\n\n{traceback.format_exc()}"))
        return history, ""

# === Gradio UI ===
with gr.Blocks() as demo:
    gr.Markdown("# ðŸ“˜ StudyMate â€” IBM Granite + Summary + Key Points + MCQs")

    with gr.Row():
        pdf_input = gr.File(
            label="Upload PDFs",
            file_count="multiple",
            file_types=[".pdf"],
            type="filepath",
        )
        process_btn = gr.Button("Process PDFs")

    status = gr.Textbox(label="Status", interactive=False)
    chatbot = gr.Chatbot(label="StudyMate Chat", height=500)
    state = gr.State([])

    with gr.Row():
        msg = gr.Textbox(placeholder="Ask a question...", label="Your question", lines=2)
        send = gr.Button("Send")
        clear = gr.Button("Clear")

    process_btn.click(process_pdfs, inputs=[pdf_input], outputs=[status, state])
    msg.submit(answer_question, inputs=[msg, state], outputs=[chatbot, msg])
    send.click(answer_question, inputs=[msg, state], outputs=[chatbot, msg])

    def _clear_all():
        return [], "", "", []
    clear.click(_clear_all, None, [chatbot, msg, status, state])

demo.launch(share=True)

